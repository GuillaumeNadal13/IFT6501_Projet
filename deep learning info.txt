Deep leaning
- uses ?? to recognize certain pattern machine learning algorithms cant (machine learning would need to be programmed
to recognize patterns (shapes) to tell them appart/ like what is a face, what are eyes etc..
- uses neural networks to process data directly
- the source of many different advancements (playing chest, go, other games beating humans, self driving cars,
recognizes spam emails)
- in deep learning you can learn features from just the raw data (feed a bunch of images to the model) then the model
will have an idea of the pattern of the lines (lines and edges*) and shapes of a face for example

- deep learning requires big data
- deep learning requires improved hardware architectures
- deep learning (new software) - tensorflow + pytorch

- the BASE of deep learning is Neural Networks
- the basics of the neural network is that it
    1- take the data as input
    2- train itself to understand the pattern in the data
    3- outputs useful predictions

    Input layer -- Hidden layers -- Output layer

forward propagation -- back propagation
1- forward propagation
    Input layer to Output layer
        inputs uses channels to between neurones and each channel have weights

        inputs are multiplied to the weights and the value is given to the hidden layer's neurone
        each neurone is associated with a numerical value called bias which is added to the input sum
        the input sum passes through the activation function which decides if 1 or 0


    Important terminologie:
        weight : tells how important is that neuron -- higher value = more important

        bias : allows for the shifting of the activation function to the right or left
            is like the neurone having a opinion to the relationship (serves to shift the activation function to the
            right or to the left (0 or 1)

2- back propagation
    Output layer to Hidden layers
        ** is what makes the algorythm learn
        the neural network evaluates is owe performance and checks if its right or wrong
            # if wrong the neural network uses the Loss function to quantify the deviation from the expected output
            # that information is sent back to the hidden layers so that the weights and biais are ajusted
                # so that network accuracy increases

Learning algorithm :
    1- Initialize parameters with random values
    2- Feed input data to network
    3- Compared predicted value with expected; then calculates the loss
    4- Perform backpropagation to propagate the loss back through the network
    5- Updates parameters based on the loss (so that total loss is reduced and a better model is obtained
    6- Iterate previous steps till loss is minimized

Activation Function :
    It introduces a non-linearity in the network and decides whether a neuron can contribute to the next layer or not

    1- Step function:
        if value greater than a certain value it activates, if not doesn't activate
        is a binary function
        doesnt work if you need to sort inputs through multiple classes (exemple)
    2 - Linear function
        cannot have multiple layers, cause derivative of y = mx+b is a constant m
    3- Sigmoid function
        the output is going to be in the range of (0, 1)
        non-linear so allows multiple layers
        gives analog outputs (not 0 and 1)
        vanishing gradient problem
    4- Tanh function
        can stack layers
        range(0, 1)
        vanishing problem
    5- Relu function
        non-linear, so can stack layers
        outputs 0 if negative value; so almost half of all the neurones will be activated (network lighter)
        range of (0, infinite) -- it could blow up the activation
        for the negative values gradient = 0; so during backpropagation the weights won't be adjusted; all neurone is
        zero state will stop responding to error ; called: dying relu problem (makes a part of the network passive
        instead of activate(what we want)
    6- Leaky Relu function
            gradient not 0; but is still less computationally intensive then tanh and sigmoid

Which to use :
    binary classification problem: sigmoid
    unsure : ReLU

Loss function :
